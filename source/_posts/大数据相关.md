---
title: 大数据相关
date: 2019-11-24 16:48:37
tags: BigData
categories: BigData
---

重点参考：

- https://github.com/CheckChe0803/BigData-Interview

<table>
    <tr>
     <th><img width="50px" src="大数据相关/hadoop.jpg"></th>
     <th><img width="50px" src="大数据相关/hive.jpg"></th>
     <th><img width="50px" src="大数据相关/spark.jpg"></th>
     <th><img width="50px" src="大数据相关/flink.png"></th>
     <th><img width="50px" src="大数据相关/hbase.png"></th>
     <th><img width="50px" src="大数据相关/kafka.png"></th>
     <th><img width="50px" src="大数据相关/zookeeper.jpg"></th>
    </tr>
<tr>
  <td align="center">Hadoop</td>
  <td align="center">Hive</td>
  <td align="center">Spark</td>
  <td align="center">Flink</td>
  <td align="center">HBase</td>
  <td align="center">Kafka</td>
  <td align="center">Zookeeper</td>
</tr>
 <tr>
     <td align="center"><a href="https://hadoop.apache.org/docs/">文档</a></td>
  <td align="center"><a href="https://hive.apache.org/">文档</a></td>
  <td align="center"><a href="https://spark.apache.org/docs/">文档</a></td>
  <td align="center"><a href="https://ci.apache.org/projects/flink/flink-docs-stable/">文档</a></td>
  <td align="center"><a href="https://hbase.apache.org/book.html">文档</a></td>
  <td align="center"><a href="https://kafka.apache.org/documentation/">文档</a></td>
  <td align="center"><a href="https://zookeeper.apache.org/">文档</a></td>
</tr>
</table>

<!--more-->

## Hadoop

### [HDFS架构？](https://hadoop.apache.org/docs/)[^2]

#### [HDFS1](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)

- 架构图

> HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.

![](大数据相关/hdfs1architecture.gif)

- [x] NameNode

NameNode 负责管理整个分布式系统的元数据，主要包括：

 - 目录树结构；
 - 文件到数据库 Block 的映射关系；

- Block 副本及其存储位置等管理数据；
- DataNode 的状态监控，两者通过段时间间隔的心跳来传递管理信息和数据信息，通过这种方式的信息传递，NameNode 可以获知每个 DataNode 保存的 Block 信息、DataNode 的健康状况、命令 DataNode 启动停止等（如果发现某个 DataNode 节点故障，NameNode 会将其负责的 block 在其他 DataNode 上进行备份）。

这些数据保存在内存中，同时在磁盘保存两个元数据管理文件：fsimage 和 editlog。

- fsimage：是内存命名空间元数据在外存的镜像文件；
- editlog：则是各种元数据操作的 write-ahead-log 文件，在体现到内存数据变化前首先会将操作记入 editlog 中，以防止数据丢失。

这两个文件相结合可以构造完整的内存数据。

- [x] Secondary NameNode

Secondary NameNode 并不是 NameNode 的热备机，而是定期从 NameNode 拉取 fsimage 和 editlog 文件，并对两个文件进行合并，形成新的 fsimage 文件并传回 NameNode，这样做的目的是减轻 NameNod 的工作压力，本质上 SNN 是一个提供检查点功能服务的服务点。

- [x] DataNode

负责数据块的实际存储和读写工作，Block 默认是64MB（HDFS2.0改成了128MB），当客户端上传一个大文件时，HDFS 会自动将其切割成固定大小的 Block，为了保证数据可用性，每个 Block 会以多备份的形式存储，默认是3份。

#### [HDFS2](https://hadoop.apache.org/docs/r2.10.0/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)

- 架构图和上面一样
- [HDFS High Availability]()

![](大数据相关/hdfs-ha.png)

| 属性                                                         |
| ------------------------------------------------------------ |
| **（1）Active NameNode 和 Standby NameNode**：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务； |
| **（2）ZKFailoverController**（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）； |
| **（3）Zookeeper 集群**：为主备切换控制器提供主备选举支持；  |
| **（4）共享存储系统**：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在**确认元数据完全同步之后才能继续对外提供服务**。 |
| **（5）DataNode 节点**：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 |

### HDFS[比较](https://cwiki.apache.org/confluence/display/HADOOP2/Roadmap)（主要体现在jdk版本，HA等）

| 比较   | 特性                          |
| ------ | ----------------------------- |
| HDFS 1 |                               |
| HDFS 2 | HADOOP，HDFS，YARN，MAPREDUCE |
| HDFS 3 | Move to JDK8+                 |



### HDFS相关的例子？

- 常用命令

```
查看文件系统的基本信息和统计信息：hdfs dfsadmin -report
建立文件夹: hadoop fs -mkdir /user/tpc-h1G
上传文件:hadoop fs -put *.tbl /user/tpc-h1G
```



### Yarn架构？（资源管理）

> The fundamental idea of YARN is to split up the functionalities of resource management and job scheduling/monitoring into separate daemons. The idea is to have a global ResourceManager (*RM*) and per-application ApplicationMaster (*AM*). An application is either a single job or a DAG of jobs.

- 原理[^1][^3]
- Yarn架构图

![](https://hadoop.apache.org/docs/r3.2.1/hadoop-yarn/hadoop-yarn-site/yarn_architecture.gif)

#### ResourceManager（RM）

RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成：

- 调度器 Scheduler

调度器根据容量、队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 **资源容器(Resource Container，也即 Container)**，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。

- 应用程序管理器 Applications Manager，ASM。

应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。

#### NodeManager（NM）

NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。

#### ApplicationMaster（AM）

提交的每个作业都会包含一个 AM，主要功能包括：

- 与 RM 协商以获取资源（用 container 表示）；

- 将得到的任务进一步分配给内部的任务；

- 与 NM 通信以启动/停止任务；

- 监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。

MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。

#### Container

Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。

### MapReduce过程？

MapReduce分为两个阶段: **Map** 和  **Ruduce**.

#### **Map阶段:**

- **input**.

 在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务

- **map**

就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行 

- **Partition**. 

需要计算每一个map的结果需要发到哪个reduce端,partition数等于reducer数.默认采用HashPartition.

- **spill**

此阶段分为sort和combine.首先分区过得数据会经过排序之后写入环形内存缓冲区.在达到阈值之后守护线程将数据溢出分区文件.

- **sort** 

在写入环形缓冲区前,对数据排序.<key,value,partition>格式排序

- **combine**(可选). 

在溢出文件之前,提前开始combine,相当于本地化的reduce操作

- **merge** 

spill结果会有很多个文件,但最终输出只有一个,故有一个merge操作会合并所有的本地文件,并且该文件会有一个对应的索引文件.

#### **Reduce阶段:**

- **copy**.

拉取数据,reduce启动数据copy线程(默认5个),通过Http请求对应节点的map task输出文件,copy的数据也会先放到内部缓冲区.之后再溢写,类似map端操作.

- **merge**

 合并多个copy的多个map端的数据.在一个reduce端先将多个map端的数据溢写到本地磁盘,之后再将多个文件合并成一个文件.  数据经过 **内存->磁盘 , 磁盘->磁盘**的过程.

- **output**

merge阶段最后会生成一个文件,将此文件转移到内存中,shuffle阶段结束

- **reduce**

开始执行reduce任务,最后结果保留在hdfs上.

#### 案例[^4]

**1.下表是一个不同年份的用电量，找出平均用电量最大的年份**

| Jan  | Feb  | Mar  | Apr  | May  | Jun  | Jul  | Aug  | Sep  | Oct  | Nov  | Dec  | Avg  |      |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 1979 | 23   | 23   | 2    | 43   | 24   | 25   | 26   | 26   | 26   | 26   | 25   | 26   | 25   |
| 1980 | 26   | 27   | 28   | 28   | 28   | 30   | 31   | 31   | 31   | 30   | 30   | 30   | 29   |
| 1981 | 31   | 32   | 32   | 32   | 33   | 34   | 35   | 36   | 36   | 34   | 34   | 34   | 34   |
| 1984 | 39   | 38   | 39   | 39   | 39   | 41   | 42   | 43   | 40   | 39   | 38   | 38   | 40   |
| 1985 | 38   | 39   | 39   | 39   | 39   | 41   | 41   | 41   | 00   | 40   | 39   | 39   | 45   |

分别实现Mapper和Reducer接口。

```java
package hadoop; 

import java.util.*; 

import java.io.IOException; 
import java.io.IOException; 

import org.apache.hadoop.fs.Path; 
import org.apache.hadoop.conf.*; 
import org.apache.hadoop.io.*; 
import org.apache.hadoop.mapred.*; 
import org.apache.hadoop.util.*; 

public class ProcessUnits {
   //Mapper class 
   public static class E_EMapper extends MapReduceBase implements 
   Mapper<LongWritable ,/*Input key Type */ 
   Text,                /*Input value Type*/ 
   Text,                /*Output key Type*/ 
   IntWritable>        /*Output value Type*/ 
   {
      //Map function 
      public void map(LongWritable key, Text value, 
      OutputCollector<Text, IntWritable> output,   
      
      Reporter reporter) throws IOException { 
         String line = value.toString(); 
         String lasttoken = null; 
         StringTokenizer s = new StringTokenizer(line,"\t"); 
         String year = s.nextToken(); 
         
         while(s.hasMoreTokens()) {
            lasttoken = s.nextToken();
         }
         int avgprice = Integer.parseInt(lasttoken); 
         output.collect(new Text(year), new IntWritable(avgprice)); 
      } 
   }
   
   //Reducer class 
   public static class E_EReduce extends MapReduceBase implements Reducer< Text, IntWritable, Text, IntWritable > {
   
      //Reduce function 
      public void reduce( Text key, Iterator <IntWritable> values, 
      OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException { 
         int maxavg = 30; 
         int val = Integer.MIN_VALUE; 
            
         while (values.hasNext()) { 
            if((val = values.next().get())>maxavg) { 
               output.collect(key, new IntWritable(val)); 
            } 
         }
      } 
   }

   //Main function 
   public static void main(String args[])throws Exception { 
      JobConf conf = new JobConf(ProcessUnits.class); 
      //任务：获取最大用电量
      conf.setJobName("max_eletricityunits"); 
      conf.setOutputKeyClass(Text.class);
      conf.setOutputValueClass(IntWritable.class); 
      conf.setMapperClass(E_EMapper.class); 
      conf.setCombinerClass(E_EReduce.class); 
      conf.setReducerClass(E_EReduce.class); 
      conf.setInputFormat(TextInputFormat.class); 
      conf.setOutputFormat(TextOutputFormat.class); 
      
      FileInputFormat.setInputPaths(conf, new Path(args[0])); 
      FileOutputFormat.setOutputPath(conf, new Path(args[1])); 
      
      JobClient.runJob(conf); 
   } 
} 
```

不建立单机环境，仅导包完成调试功能：运行和调试MapReduce程序只需要有相应的Hadoop依赖包就行，可以完全当成一个普通的JAVA程序。

**2.排序：order by**

**3.去重：distinct**

**4.多表查询**

**5.倒排索引**

(ps:spark经典案例[^8])

### Yarn 调度MapReduce？

Yarn采用的双层调度框架，RM将资源分配给AM,AM再将资源进一步分配给Task,资源不够时会为TASK预留，直到资源充足。

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-core</artifactId>
        <version>1.2.1</version>
    </dependency>
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>2.7.2</version>
    </dependency>
</dependencies>
```



### hdfs写流程？

<img src="大数据相关/hdfs写流程.png"/>

1. Client 调用 DistributedFileSystem 对象的 `create` 方法，创建一个文件输出流（FSDataOutputStream）对象；
2. 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息；
3. 通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block；
4. DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点；
5. DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功；
6. 完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 `close` 方法，完成文件写入；
7. 调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。

### hdfs读流程？

![](大数据相关/hdfs读流程.png)

1. Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息；
2. NameNode 返回存储的每个块的 DataNode 列表；
3. Client 将连接到列表中最近的 DataNode；
4. Client 开始从 DataNode 并行读取数据；
5. 一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。

### hdfs创建一个文件的流程？(类的调用过程)

- Apache Hadoop HDFS 2.9.1 API[^5]

1. 客户端通过ClientProtocol协议向RpcServer发起创建文件的RPC请求。
2. FSNamesystem封装了各种HDFS操作的实现细节，RpcServer调用FSNamesystem中的相关方法以创建目录。
3. 进一步的，FSDirectory封装了各种目录树操作的实现细节，FSNamesystem调用FSDirectory中的相关方法在目录树中创建目标文件，并通过日志系统备份文件系统的修改。
4. 最后，RpcServer将RPC响应返回给客户端。

### hadoop1.x 和hadoop 2.x 的区别？

- **资源调度方式的改变**

在1.x, 使用Jobtracker负责任务调度和资源管理,单点负担过重,在2.x中,新增了yarn作为集群的调度工具.在yarn中,使用ResourceManager进行 资源管理, 单独开启一个Container作为ApplicationMaster来进行任务管理.

- **HA模式**

在1.x中没有HA模式,集群中只有一个NameNode,而在2.x中可以启用HA模式,存在一个Active NameNode 和Standby NameNode.

- **HDFS Federation**

Hadoop 2.0中对HDFS进行了改进，使NameNode可以横向扩展成多个，每个NameNode分管一部分目录，进而产生了HDFS Federation，该机制的引入不仅增强了HDFS的扩展性，也使HDFS具备了隔离性

#### hadoop1.x：

![](大数据相关/hadoop1.jpg)

#### hadoop2.x：

![](大数据相关/hadoop2.jpg)



### hadoop HA介绍？

- HDFS High Availability[^6]

1. **Active NameNode 和 Standby NameNode**：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务；
2. **ZKFailoverController（主备切换控制器，FC）**：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）；
3. **Zookeeper 集群**：为主备切换控制器提供主备选举支持；
4. **共享存储系统**：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在**确认元数据完全同步之后才能继续对外提供服务**。
5. **DataNode 节点**：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。

### hadoop的常用配置文件？

**在TPC-H的测评实验中，使用配置文件见github.com/maomao1994/TPC-H[^7]**

1. **hadoop-env.sh**: 用于定义hadoop运行环境相关的配置信息，比如配置JAVA_HOME环境变量、为hadoop的JVM指定特定的选项、指定日志文件所在的目录路径以及master和slave文件的位置等；
2. **core-site.xml**: 用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录以及用于rack-aware集群中的配置文件的配置等，此中的参数定义会覆盖core-default.xml文件中的默认配置；
3. **hdfs-site.xml**: HDFS的相关设定，如文件副本的个数、块大小及是否使用强制权限等，此中的参数定义会覆盖hdfs-default.xml文件中的默认配置；
4. **mapred-site.xml**：HDFS的相关设定，如reduce任务的默认个数、任务所能够使用内存的默认上下限等，此中的参数定义会覆盖mapred-default.xml文件中的默认配置；
5. **yarn-site.xml**
6. **~/hadoop/etc/hadoop/slaves**

### 小文件过多会有什么危害,如何避免？

Hadoop上大量HDFS元数据信息存储在NameNode内存中,因此过多的小文件必定会压垮NameNode的内存.

每个元数据对象约占150byte，所以如果有1千万个小文件，每个文件占用一个block，则NameNode大约需要2G空间。如果存储1亿个文件，则NameNode需要20G空间.

显而易见的解决这个问题的方法就是合并小文件,可以选择在客户端上传时执行一定的策略先合并,或者是使用Hadoop的CombineFileInputFormat<K,V>实现小文件的合并

### 启动hadoop集群会分别启动哪些进程,各自的作用？

1. **NameNode：**
   - 维护文件系统树及整棵树内所有的文件和目录。这些信息永久保存在本地磁盘的两个文件中：命名空间镜像文件、编辑日志文件
   - 记录每个文件中各个块所在的数据节点信息，这些信息在内存中保存，每次启动系统时重建这些信息
   - 负责响应客户端的   数据块位置请求  。也就是客户端想存数据，应该往哪些节点的哪些块存；客户端想取数据，应该到哪些节点取
   - 接受记录在数据存取过程中，datanode节点报告过来的故障、损坏信息

2. **SecondaryNameNode(非HA模式)：**
   - 实现namenode容错的一种机制。定期合并编辑日志与命名空间镜像，当namenode挂掉时，可通过一定步骤进行上顶。(**注意 并不是NameNode的备用节点**)
3. **DataNode：**
   - 根据需要存取并检索数据块
   - 定期向namenode发送其存储的数据块列表
4. **ResourceManager：**
   - 负责Job的调度,将一个任务与一个NodeManager相匹配。也就是将一个MapReduce之类的任务分配给一个从节点的NodeManager来执行。
5. **NodeManager：**
   - 运行ResourceManager分配的任务，同时将任务进度向application master报告

6. **JournalNode(HA下启用):**
   - 高可用情况下存放namenode的editlog文件

## HIVE

### hive 内部表和外部表的区别？

### Hive中 sort by / order by / cluster by / distribute by 的区别？

### hive的metastore的三种模式？

### hive 中 join都有哪些？

### Impala 和 hive 的查询有哪些区别？

### Hive中大表join小表的优化方法]？

### Hive Sql 是怎样解析成MR job的?

### Hive UDF简单介绍？

### SQL题: 按照学生科目分组, 取每个科目的TopN？

### SQL题: 获取每个用户的前1/4次的数据？







## Spark

### spark 的运行架构？

### 一个spark程序的执行流程？

### spark的shuffle介绍？

### Spark的 partitioner 都有哪些?

### spark 有哪几种join？

### RDD有哪些特点？

### 讲一下宽依赖和窄依赖？

### Spark中的算子都有哪些？

### RDD的缓存级别都有哪些？

### RDD 懒加载是什么意思？

### 讲一下spark的几种部署方式？

### spark on yarn 模式下的 cluster模式和 client模式有什么区别？

### spark运行原理,从提交一个jar到最后返回结果,整个过程？

### spark的stage是如何划分的？

### spark的rpc: spark2.0为什么放弃了akka 而用netty？

### spark的各种HA,  master/worker/executor/driver/task的ha？

### spark的内存管理机制,spark 1.6前后分析对比, spark2.0 做出来哪些优化？

### 讲一下spark 中的广播变量？

### 什么是数据倾斜,怎样去处理数据倾斜？

### 分析一下一段spark代码中哪些部分在Driver端执行,哪些部分在Worker端执行？



## Flink

### 讲一下flink的运行架构？

### 讲一下flink的作业执行流程？

### flink具体是如何实现exactly once 语义？

### flink 的 window 实现机制？

### flink的window分类？

### flink 的 state 是存储在哪里的？

### flink是如何实现反压的？

### flink的部署模式都有哪些？

### 讲一下flink on yarn的部署？

### flink中的时间概念 , eventTime 和 processTime的区别？

### flink中的session Window怎样使用？

### flink中的session Window怎样使用









## HBase

### 讲一下 Hbase 架构？

### hbase 如何设计 rowkey？

### 讲一下hbase的存储结构,这样的存储结构有什么优缺点？

### hbase的HA实现,zookeeper在其中的作用？

### HMaster宕机的时候,哪些操作还能正常工作？

### 讲一下hbase的写数据的流程？

### 讲一下hbase读数据的流程？









## Kafka

### 讲一下 kafka 的架构？

### kafka 与其他消息组件对比？

### kafka 实现高吞吐的原理？

### kafka怎样保证不重复消费？

### kafka怎样保证不丢失消息？

### kafka 与 spark streaming 集成,如何保证 exactly once 语义？

### ack 有哪几种, 生产中怎样选择？

### 如何通过 offset 寻找数据？

### 如何清理过期数据？

### 1条message中包含哪些信息？

### 讲一下zookeeper在kafka中的作用？

### kafka 可以脱离 zookeeper 单独使用吗？

### kafka有几种数据保留策略？

### kafka同时设置了7天和10G清除数据,到第5天的时候消息到达了10G,这个时候kafka如何处理？



## Zookeeper

### zookeeper是什么,都有哪些功能？

### zk 有几种部署模式？

### zk 是怎样保证主从节点的状态同步？

### 说一下 zk 的通知机制？

### zk 的分布式锁实现方式？

### zk 采用的哪种分布式一致性协议? 还有哪些分布式一致性协议？

### 讲一下leader 选举过程？



## 引用

[^1]:Vinod  Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, Sharad Agarwal, &  Eric Baldeschwieler. (2013). Apache Hadoop YARN: yet another resource  negotiator. *Proceedings of the 4th annual Symposium on Cloud Computing*. ACM.
[^2]: https://hadoop.apache.org/docs/

[^3]: Vavilapalli, V. K., Murthy, A. C., Douglas, C., Agarwal, S., Konar, M., Evans, R., ... & Saha, B. (2013, October). Apache hadoop yarn: Yet another resource negotiator. In *Proceedings of the 4th annual Symposium on Cloud Computing* (p. 5). ACM.
[^4]: https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm

[^5]: http://hadoop.apache.org/docs/r2.9.1/hadoop-project-dist/hadoop-hdfs/api/overview-summary.html 
[^6]:https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html
[^7]: https://github.com/maomao1994/TPC-H/
[^8]: [http://www.damaoguo.site/2019/03/30/spark%E7%BB%8F%E5%85%B8%E6%A1%88%E4%BE%8B/](http://www.damaoguo.site/2019/03/30/spark经典案例/)


